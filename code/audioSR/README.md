In order to create and train a network on a dataset, you need to:  

1. Generate a binary file from the source dataset. It's easiest if you structure your data as in the TIMIT dataset, although that's not really needed. Just make sure that the wav and its corresponding phn file have the same path except for the extension. Otherwise they won't get matched and your labels will be off.    
    - WAV files, 16kHz sampling rate, folder structure `dataset/TRAIN/speakerName/videoName/`. 
        Each videoName/ directory contains a `videoName.wav` and `videoName.phn`. 
        The phn contains the audio sample (@16kHz) numbers where each phoneme starts and ends.   
    - If your files are in a different format, you can use functions from fixDataset/ to: (use transform.py, with the appropriate arguments, see bottom of file)   
        - fix wav headers, resample wavs. Store them under `dataRoot/dataset/fixed(nbPhonemes)/`  
        `transform.py phonemes -i dataRoot/TIMIT/original/ -o dataRoot/TIMIT/fixed`
        - fix labelfiles: replace phonemes (eg to use a reduced phoneme set; I used the 39 phonemes from Lee and Hon (1989)).  Stored next to fixed wavs, under `root/dataset/fixed(nbPhonemes)/`  
        - create a MLF file (like from HTK tool, and as used in the TCDTIMIT dataset)   
        - the scripts should be case-agnostic, but you can convert lower to uppercase and vice versa by running `find . -depth -print0 | xargs -0 rename '$_ = lc $_'` in the root dataset directory (change 'lc' to 'uc to convert to upper case). Repeat until you get no more output.  
    - Then set variables in datasetToPkl.py (source and target dir, nbMFCCs to use etc), and run the file   
        - the result is stored as `root/dataset/binary(nbPhonemes)/dataset/dataset_nbPhonemes_ch.pkl`. eg root/TIMIT/binary39/TIMIT/TIMIT_39_ch.pkl  
        - the mean and std_dev of the train data are stored as `root/dataset/binary_nbPhonemes/dataset_MeanStd.pkl`. It's useful for normalization when evaluating. 
1. Use RNN.py to start training. Its functions are implemented in RNN_tools_lstm.py, but you can set the parameters from RNN.py.    
    - set location of pkl generated by datasetToPkl.py  
    - specify number of LSTM layers and number of units per layer  
    - use bidirectional LSTM layers   
    - add some dense layers (though it did not improve performance for me)  
    - learning rate and decay (LR is updated at end of RNN_tools_lstm.py). It's decreased if the performance hasn't improved for some time.    
    
    - it will automatically give the model a name based on the specified parameters. A log file, the model parameters and a pkl file containing training info (accuracy, error etc for each epoch) are stored as well. 
      The storage location is`root/dataset/results`  
        
1. You can use evaluateManyDatasets.py to test your trained network on any (labeled) dataset. (I used TCDTIMIT and TIMIT).  
    - Set the correct network parameters and root dir; it will build the model name and check if the model has been trained.  
    - To increase evaluation speed, preprocessed evaluation data is stored in pkl format, so it doesn't need to be preprocessed each evaluation.  
  
The TIMIT dataset is non-free and available from [https://catalog.ldc.upenn.edu/LDC93S19](https://catalog.ldc.upenn.edu/LDC93S1).    
The TCDTIMIT dataset is publicly available from [https://sigmedia.tcd.ie/TCDTIMIT/](https://sigmedia.tcd.ie/TCDTIMIT/).  
I recommend to use my repo [TCDTIMITprocessing](https://github.com/matthijsvk/TCDTIMITprocessing) to download, exctract the database. You can use `extractTCDTIMITaudio.py` to get the phoneme and wav files.

It also contains scripts for lipreading: extract faces, mouths etc from the video depending on label files.  
It also contains scripts for lipreading: extract faces, mouths etc from the video depending on label files.  
``
